{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\HP\n",
      "[nltk_data]     PAVILION\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/dataset.csv', delimiter=';')\n",
    "df_stopwords = pd.read_csv('stopwords-id.csv', header=None, names=['stopword'])\n",
    "df_slang = pd.read_csv('kamus-singkatan.csv', delimiter=';', names=['singkatan', 'kata'])\n",
    "df_lexicon = pd.read_csv('lexicon.csv')\n",
    "df_corpus = pd.read_csv('corpus.csv')\n",
    "\n",
    "stemmer = StemmerFactory().create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    global df_slang, df_stopwords, df_lexicon, df_corpus\n",
    "    def cleaning(text):\n",
    "        text = text.replace('-ness', '').replace('-jualness', '')\n",
    "        text = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", text)\n",
    "        text = re.sub(r'^RT[\\s]+', '', text)\n",
    "        text = re.sub(r'/n', ' ', text)\n",
    "        text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', ' ', text)\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        text = re.sub(r'[0-9]+', '', text)\n",
    "        text = re.sub(r'(?<!\\bunnes)(\\w)(\\1+)(?=\\s|[\\.,!])', r'\\1', text)\n",
    "        text = text.strip(' ')\n",
    "        text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "        text = text.lower()  \n",
    "        return text\n",
    "    \n",
    "    def replace_word_elongation(text):\n",
    "        elongated_words = re.findall(r'\\b\\w*(?:(\\w)\\1{2,})\\w*\\b', text)\n",
    "        for word in elongated_words:\n",
    "            replacement = word[0]\n",
    "            text = re.sub(r'\\b' + re.escape(word) + r'\\b', replacement, text)\n",
    "        return text\n",
    "    \n",
    "    def tokenize(text):\n",
    "        text = word_tokenize(text)\n",
    "        return text\n",
    "    \n",
    "    def translate_slang_list(text_list):\n",
    "        global df_slang\n",
    "        translated_list = []\n",
    "        for text in text_list:\n",
    "            words = text.split()\n",
    "            translated_words = []\n",
    "            for word in words:\n",
    "                if word in df_slang['singkatan'].tolist():\n",
    "                    translated_words.append(df_slang[df_slang['singkatan'] == word]['kata'].values[0])\n",
    "                else:\n",
    "                    translated_words.append(word)\n",
    "            translated_list.append(' '.join(translated_words))\n",
    "        return text_list\n",
    "    \n",
    "    def remove_stopwords(text):\n",
    "        global df_stopwords\n",
    "        if isinstance(text, list):\n",
    "            filtered_words = [word for word in text if word.lower() not in df_stopwords['stopword'].str.lower().values]\n",
    "            return filtered_words\n",
    "        else:\n",
    "            return text\n",
    "    \n",
    "    def lemmatization(tokens):\n",
    "        factory = StemmerFactory()\n",
    "        stemmer = factory.create_stemmer()\n",
    "        lemmatized_tokens = [stemmer.stem(token) for token in tokens]\n",
    "        lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "        return text\n",
    "    \n",
    "    def labelling(text):\n",
    "        global df_lexicon, df_corpus\n",
    "        words = text.lower().split()\n",
    "        score = 0\n",
    "        for word in words:\n",
    "            if word in df_lexicon['word'].values:\n",
    "                weight = df_lexicon.loc[df_lexicon['word'] == word, 'weight'].values[0]\n",
    "                score += weight\n",
    "        if score > 0:\n",
    "            return 'positif'\n",
    "        elif score < 0:\n",
    "            service_words = df_corpus['kata'].values\n",
    "            for service_word in service_words:\n",
    "                if service_word in text:\n",
    "                    return 'negatif'\n",
    "            return 'netral'\n",
    "        else:\n",
    "            return 'netral'\n",
    "\n",
    "    \n",
    "    text = cleaning(text)\n",
    "    text = replace_word_elongation(text)\n",
    "    tokens = tokenize(text)\n",
    "    tokens = translate_slang_list(tokens)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    lemmatized_text = lemmatization(tokens)\n",
    "    label = labelling(lemmatized_text)\n",
    "    \n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = data['full_text'].apply(preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              full_text    label\n",
      "228   -ness emang bener ya kalo kita ikut program mb...  negatif\n",
      "233   @ndak_tauu @unnesmenfess kalau lantip biasanya...  negatif\n",
      "287      @tanyarlfes Sbg alumni Unnes merasa TertamparðŸ¤­  negatif\n",
      "331   Pricelist Update!  Unnes - St. Poncol 20k Unne...  negatif\n",
      "398   @unnesmenfess Jurnal terindeks scopus bs downl...  negatif\n",
      "...                                                 ...      ...\n",
      "6515  -ness info kost cowok exclusive yang bisa park...  negatif\n",
      "6535  -ness butuh anjem nanti sore dari kampus ke ko...  negatif\n",
      "6576  -ness info kos bulanan di sekitar unnes, fasil...  negatif\n",
      "6641  berarti aku nggak usah ngumpulin, kan? ini gap...  negatif\n",
      "6645  @lhchcn Buset, jelass. Tiap anak beda beda pw ...  negatif\n",
      "\n",
      "[109 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "negatif_rows = data[data['label'] == 'negatif']\n",
    "print(negatif_rows[['full_text', 'label']])\n",
    "negatif_rows[['full_text', 'label']].to_csv('negatif_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X = data['full_text']  \n",
    "y = data['label']  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "svm_model = SVC(kernel='rbf', C=100, gamma=0.01)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "predictions = svm_model.predict(X_test_tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
